- config.py : OK
    test_config.py : OK
- custom_objects.py : OK 
    test_custom_objects.py : OK mais on pourrait rajouter swish
- efficientmixnet.py : OK, ça compile avec MDconv et ça fit... 
- scripts qui automatisent préprocessing, training, eval : OK
- préparer les données - > batch training : OK
- modifier load pileup pour avoir la même logique que batch_training : OK
- RADAM + LOOKAHEAD optimizers -> adapter scripts en conséquences : OK
- montrer qu'on a de meilleurs résultats et faire code propre + factorisé : Ok, avec lookahead + Adam + mixed depthwise, en entrainant sur chr1-2-21 downsamplés 
                                                                            on a 99.9% recall, 97.8% f2 score, 94.4% acc et 91% precision
- vérifier vite fait si les données du test sont bien balanced / même distrib que le training : OK, 13672 - 13672 downsampled sinon 45685 - 13672 chr22 normal
- Gerer les logs pour les répartir en différents fichiers : OK -> /logs_date/...
- modifier training_utils : OK
- RESTRUCTURER TOUT LE CODE FAIT PENDANT LE STAGE + DATA : OK, ptet qq trucs à trier encore vis à vis des siameses nets par ex
- RESTRUCTURER LE PROJET : OK
- FAIRE BEAU NOTEBOOK AVEC DOC + EXEMPLES DAPPELS SCRIPTS : OK 
- Implémenter en Keras Instance Enhancement Batch Normalization: an Adaptive Regulator of Batch Noise  https://arxiv.org/pdf/1908.04008.pdf : OK
- encodage string : OK
- Coder les différentes activations -> config + efficientmixnet : OK
- Nb : l'error inbouund nodes not found venait de ma fonction activation : OK, mis une ocuche Activation() à la place

- Coder Batchnorm pour avoir le choix : OK MAIS ENCORE DES ERREURS je sais pas d'où ça vient vu que la layer a l'air de fonctionner : OK c bon : oublie d'un s à inputs et oublie d'un int() dans la config




à faire : 

- Le code est déjà bien factorisé, mais p-e regarder à comment simplifier la génération des modèles EfficientNetBX (class, factory ...)
- Revoir la politique des logs pour l'instantiation du modèle ptet juste stocker la config qq part : truc intermediaire : stock la liste des blocs encodés, essayer de voir si on peut stocker les paras utilisés
- corriger les scripts training / test / inclure images doc etc du repo V2
- docstring / documentation ++, revoir nom des variables et tout
- ablation study ou au moins tuto pour le faire
